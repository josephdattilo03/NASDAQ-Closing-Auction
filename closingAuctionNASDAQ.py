# -*- coding: utf-8 -*-
"""CIS5450ProjectPersonal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-fCwlLfJr8Sb3aB-hNR3c9oKVF0RqLCk

# Predictive Modeling for NASDAQ Closing Auctions

## Introduction
This project is a culmination of our efforts to understand and model the NASDAQ closing auction, where the final prices of stocks are determined. Our aim is to unravel the intricacies behind the short-term price movements during the auction's critical 10-minute window.

## Project Objective
- **Goal**: Predict the forward price movements during the NASDAQ's Closing Cross Auction.

## Approach Overview
Our systematic approach includes:

  ### Data Analysis
  - Conducting a thorough examination of the dataset to glean insights into the auction process and key influencing factors.

  ### Data Preparation
  - Cleaning and preprocessing the dataset to ensure data integrity and readiness for modeling.

  ### Feature Engineering
  - Developing new features from financial indicators and auction dynamics to enhance our predictive model.

  ### Technical Indicators
  - Integrating established technical indicators such as EMA, VMA, and Volatility.

  ### Predictive Modeling
  - Employing advanced machine learning techniques to build and train our predictive model.

  ### Model Evaluation
  - Using rigorous metrics to evaluate the model's performance and identify opportunities for refinement.

By following this structured approach, we aim to build a model that not only predicts with accuracy but also offers valuable insights into the closing auction mechanics.

## Initial EDA

### Initial Data Loading
We begin by importing necessary libraries and loading the dataset. A preliminary look at the data helps us understand the basic features available for our analysis.
"""

import matplotlib.pyplot as plt
from google.colab import drive
from sklearn.impute import KNNImputer
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
import os
import numpy as np
import tensorflow as tf
import random as python_random
from keras import Sequential
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.layers import Input, Dense, LSTM, Dropout
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler
import torch
from xgboost import XGBRegressor, plot_importance
drive.mount('/content/drive', force_remount=True)
file_path = '/content/drive/MyDrive/train.csv'
df = pd.read_csv(file_path)

df.head()

"""### Data Summary
A quantitative overview provides insights into the unique identifiers within our dataset such as `stock_id`, `date_id`, and `time_id`. We also examine the distribution of key numerical features.

"""

# Number of rows

row_num = df.shape[0]
print("Number of rows:", row_num)

# Quantitative analysis

unique_seconds_in_bucket = df['seconds_in_bucket'].unique()
unique_stock_ids = df['date_id'].unique()
unique_time_ids = df['time_id'].unique()
unique_stock_ids = df['stock_id'].unique()

print("Unique Seconds in Bucket:", unique_seconds_in_bucket)
print("Unique Date IDs:", unique_stock_ids)
print("Unique Time IDs", unique_time_ids)
print("Number of stocks", len(unique_stock_ids))

"""From our analysis of time IDs, we can determine that we have been given the movement of 200 stocks over 10 minutes for 480 days."""

df.describe()

"""All of the data that we will be using to train the model is already in integer or float form except for the row_id, which is unnecessary to make predictions."""

df = df.drop(columns=['row_id'])
df.dtypes

"""In order to save CPU memory, we convert our floats from float64 to float32."""

# List of columns to convert to float32
columns_to_convert = ['reference_price', 'matched_size', 'far_price', 'near_price',
                      'bid_price', 'bid_size', 'ask_price', 'ask_size',
                      'wap', 'target', 'imbalance_size']

# Convert the columns to float32
df[columns_to_convert] = df[columns_to_convert].astype('float32')
df.dtypes

"""### Cleaning and Encoding

Now that we have a more clear idea of how our data is being presented, we can add features and make changes to `df` that will make our data more presentable to a model.


Lets begin by addressing our nulls:
"""

df.isnull().sum()

"""Analyzing the nulls in the dataset, we determine that the far price and near price are only released after 3:55.

We can also show that the movement over time for each stock changes dramatically after the 3:55 marks. We can conclude that at the release of the near and far prices the matched size and imbalance size change drastically.

When the market stops accepting Market on Close Orders at 3:55, it seems like trading strategy changes to matching as much as possible and maximize the amount of crosses at the end of the auction.

The strategy change gives use reason to logically seperate the prediction process for stocks before and after the 3:55 mark.


To seperate the logic for stocks before 3:55 and stocks after, we add a seperate 'boolean' (0 or 1) class called is_355.
"""

df.describe()

bucket_grouped = df.groupby('seconds_in_bucket').mean()
plt.figure(figsize=(12, 6))
plt.plot(bucket_grouped['imbalance_size'])
plt.title('Average Imbalance Size Over Seconds in Bucket')
plt.xlabel('Seconds in Bucket')
plt.ylabel('Average Imbalance Size')
plt.show()

bucket_grouped = df.groupby('seconds_in_bucket').mean()
plt.figure(figsize=(12, 6))
plt.plot(bucket_grouped['matched_size'])
plt.title('Average Matched Size Over Seconds in Bucket')
plt.xlabel('Seconds in Bucket')
plt.ylabel('Average Matched Size')
plt.show()

"""Unfortunatly, we still have some nulls in our dataframe. In order to correct further lets use an imputer on each grouping of stocks. We need to ensure that the imputer acts on each stock individually and does not consider data from other stocks when calculating an intermediate value.

$$ $$
We will use the KNN Imputer because while the computation may be more time consuming there is not an unreasonable amount of data to replace
"""

df['is_355'] = df['seconds_in_bucket'].apply(lambda x: 1 if x >= 300 else 0)
df.loc[df['is_355'] == 0, ['near_price', 'far_price']] = 0

def knn_interpolate_group(group):
    imputer = KNNImputer(n_neighbors=2)
    group['far_price'] = imputer.fit_transform(group[['far_price']])

    return group

df = df.groupby('stock_id', group_keys=False).apply(knn_interpolate_group)

#df.describe()

"""Observing the rest of the null values, using an imputer won't be that efficient because what is left to remove is not a significant amount of data nor a significant timeframe. Therefore we will drop these columns."""

df[df.isnull().any(axis=1)]

df = df.dropna()
df.head()

"""##Feature Engineering

In our effort to refine the predictive model for NASDAQ closing auction price movements, we have engineered several new features that encapsulate different market dynamics:

### Total Volume
Found by aggregating the size of the ask (sell orders) and bid (buy orders). This figure represents the total quantity of shares offered for sale and purchase, providing insight into the market's liquidity at any given moment.

### Imbalance Ratio
 Derived from the disparity between the `imbalance_size` and `matched_size`. It provides a normalized view of the imbalance's relative magnitude compared to the volume of matched trades.
"""

# total volume --> summing the ask size and bid size
df["volume"] = df["ask_size"] + df["bid_size"]

# relative imbalance between matched and total size
df["imbalance_ratio"] = (df["imbalance_size"] - df["matched_size"]) / (df["matched_size"] + df["imbalance_size"])

# difference between ask price and bid price
df["price_spread"] = df["ask_price"] - df["bid_price"]

# product of imbalance size and bid-ask spread
df["price_pressure"] = df["imbalance_size"] * (df["price_spread"])

"""### Exponential Moving Average (EMA) Feature Engineering

The Exponential Moving Average (EMA) is a crucial feature in time series analysis, particularly in financial markets. It serves as a technical indicator that can help predict future price movements by smoothing out price data over a specified period. Here's why we incorporate EMA into our feature set:

#### Significance of EMA:
- **Trend Identification**: EMA helps identify the prevailing trend in stock prices. A rising EMA indicates an uptrend, while a falling EMA suggests a downtrend.
- **Responsiveness**: Unlike the Simple Moving Average (SMA), EMA gives more weight to recent prices, making it more responsive to recent price changes. This property is particularly useful in a fast-paced auction environment where recent price movements can be indicative of imminent trends.
- **Support and Resistance Levels**: EMAs can act as support or resistance levels in price charts. Stocks may experience a rebound when approaching the EMA line, or the EMA can act as a ceiling that the price struggles to break through.

#### Engineering EMA:
To construct the EMA feature for our dataset:
1. **Granularity**: We compute the EMA for each stock (`stock_id`) within each trading day (`date_id`) to maintain granular insights and avoid any signal dilution that would arise from aggregating across different days or stocks.
2. **Span Selection**: The span value chosen for calculating EMA represents the number of periods over which the average is computed. In our case, with a period defined as 10 seconds, we use various span values to capture short-term and long-term trends.
3. **Grouped Calculation**: We ensure that the EMA for a given stock on a given day is influenced only by the price movements (`wap`) within that specific day. This grouping maintains the integrity of the temporal sequence and avoids lookahead bias.

### Usage in Model:
- We use EMA as a feature in our model to provide a smoothed representation of price movements.
- By including EMA, the model can better understand the momentum and trend direction, which are essential in predicting the short-term price movements during the auction period.

"""

# Exponential Moving Average for Wap
def calculate_ema(dataset, time_column='date_id', stock_column='stock_id', span_values=[13, 26, 54], columns_to_average=['wap', 'reference_price', 'volume']):

     if columns_to_average is None:
         columns_to_average = dataset.select_dtypes(include='number').columns.tolist()

     new_columns = []
     for span_value in span_values:
         for column in columns_to_average:
             ema_column_name = f'{column}_EMA_{span_value}'
             # new_columns.append(dataset.groupby(time_column)[column].transform(lambda x: x.ewm(span=span_value, min_periods=1).mean()))
             dataset[ema_column_name] = dataset.groupby([stock_column, time_column])[column] \
                                               .transform(lambda x: x.ewm(span=span_value, min_periods=1).mean())
     return dataset


df = calculate_ema(df, time_column='date_id')
df.head()

stock_id_to_plot = 10
date_id_to_plot = 10

# Filter the DataFrame for the specified stock ID and date ID
stock_data = df[(df['stock_id'] == stock_id_to_plot) & (df['date_id'] == date_id_to_plot)]

# Ensure the data is sorted by 'seconds_in_bucket'
stock_data_sorted = stock_data.sort_values(by='seconds_in_bucket')

# Plot settings
plt.figure(figsize=(15, 5))

columns_to_plot = ['wap','reference_price', 'volume']
plt.figure(figsize=(18, 6))


for i, column in enumerate(columns_to_plot, 1):
    plt.subplot(1, 3, i)  # 1 row, 3 columns, ith subplot
    plt.plot(stock_data_sorted['seconds_in_bucket'], stock_data_sorted[column], label=f'{column} (Raw)', color='black', alpha=0.5)
    plt.plot(stock_data_sorted['seconds_in_bucket'], stock_data_sorted[f'{column}_EMA_13'], label=f'{column} EMA 13', linestyle='--', color='green')
    plt.plot(stock_data_sorted['seconds_in_bucket'], stock_data_sorted[f'{column}_EMA_26'], label=f'{column} EMA 26', linestyle='--', color='blue')
    plt.plot(stock_data_sorted['seconds_in_bucket'], stock_data_sorted[f'{column}_EMA_54'], label=f'{column} EMA 54', linestyle='--', color='red')

    plt.xlabel('Seconds in Bucket')
    plt.ylabel(f'{column} Value')
    plt.title(f'EMA Analysis for {column}')
    plt.legend()

# Adjust layout to prevent overlap
plt.tight_layout()
plt.show()

"""These particular columns that we calculated EMA on were found to be important values when we run all features on a KGBoost model and graph feature importance (done below). The span values were chosen based on the total number of periods for each `date_id`, which is 54. We see that larger windows produce a smoother EMA, whereas shorter span windows are much more seneitive to short-term fluctuation. The KGBoost will similarly inform us on which span winows will provide us with the most important features derived from EMA for our model.

### Rolling Standard Deviation (Volatility)

Rolling Standard Deviation is a commonly used technical indicator in financial analysis, particularly for assessing price volatility in financial markets. It quantifies the degree of price variability or dispersion in a time series of data points. In the context of trading and investment, volatility is an essential factor as it can provide insights into the potential risk and uncertainty associated with asset prices.


### Rolling Standard Deviation for WAP

In our model, we use the rolling standard deviation of the Weighted Average Price (WAP) to assess the volatility of each stock. Here's how it's calculated:

- We group the data by 'stock_id' using `df.groupby('stock_id')`.

- For each stock, we calculate the rolling standard deviation of the WAP using the `transform` function. The rolling window is set to 60 time periods (which may represent minutes, seconds, etc., depending on the dataset's time granularity).

- The rolling standard deviation provides a measure of how much the WAP values for a particular stock fluctuate over the specified time window. A higher standard deviation indicates greater price variability, while a lower standard deviation suggests more stable prices.
"""

df['rolling_std_wap'] = df.groupby('stock_id')['wap'].transform(lambda x: x.rolling(window=54).std())

"""####Rolling Standard Deviation for Matched Size

In our model, we use the rolling standard deviation of the `matched_size` to assess the volatility of the auction book. The `matched_size` column contains the number of shares at any point in time that can be matched from the bid/ask columns (they represent the number of shares where there is a buyer and seller who agree on a middle-ground `reference_price`). Here's how it's calculated:

We group the data by 'stock_id' using df.groupby('stock_id').

For each stock, we calculate the rolling standard deviation of the `matched_size` using the transform function. The rolling window is set to 54 time periods (which may represent minutes, seconds, etc., depending on the dataset's time granularity).

The rolling standard deviation provides a measure of how much the `matched_size` values for a particular stock fluctuate over the specified time window. A higher standard deviation indicates greater price variability, while a lower standard deviation suggests more stable prices.
"""

df['rolling_std_matched_size'] = df.groupby('stock_id')['matched_size'].transform(lambda x: x.rolling(window=54).std())

"""#### Rolling Standard Deviation for Reference Price

In our model, we use the rolling standard deviation of the `reference_price` to gauge the price stability of each stock. Here's how it's calculated:

We group the data by `stock_id` using df.groupby(`stock_id`).

For each stock, we compute the rolling standard deviation of the `reference_price` using the transform function. The rolling window is set to 54 time periods representing the 54 periods present in each day (since there are 54, 10-second intervals in each `date_id`).

The rolling standard deviation serves as a metric to assess how the `reference_price` values for a particular stock change within the specified time window. A higher standard deviation suggests greater price variability, indicating potential price fluctuations, while a lower standard deviation indicates more stable prices.
"""

df['rolling_std_reference_price'] = df.groupby('stock_id')['reference_price'].transform(lambda x: x.rolling(window=54).std())

"""### Volume-Weighted Average

The VWAP is a trading benchmark that gives the average price a stock has traded at throughout a day, based on both volume and price. It is important because it provides traders with insight into both the trend and value of a security.

##### Calculation of VWAP with Reference Price:
- This particular implementation of VWAP multiplies the `wap` (Weighted Average Price) by the `reference_price`, summing this product over time and then dividing by the cumulative sum of `volume`.
- This variation emphasizes the `reference_price`, integrating it into the VWAP calculation to reflect the average price at which the stock was intended to trade versus its actual trading price.
"""

#Volume Reference Price
def calculate_vwap(df):
    df['vwap_reference_price'] = (df['wap'] * df['reference_price']).cumsum() / df['volume'].cumsum()
    return df
df = calculate_vwap(df)
df.head()

"""##### Calculation of VWAP with Matched Size:

- In this computation, the `wap` is multiplied by the `matched_size`, which represents the volume of stocks that have been matched in trades.
- It then takes the cumulative sum of this product and divides by the cumulative sum of `volume`, providing a VWAP that is weighted towards the volume of stocks that were actually traded.
"""

#Volume matched_size
def calculate_vwap(df):
    df['vwap_matched_size'] = (df['wap'] * df['matched_size']).cumsum() / df['volume'].cumsum()
    return df
df = calculate_vwap(df)
df.head()

"""### Feature Importance using XGBoost
Now, we're using an XGBoost model in order to graph feature importance of our dataframe. We'll use this information to inform our feature selected for the LSTM model.
"""

# X = df.drop(columns=['target'])
# y = df['target']
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# model = XGBRegressor()
# model.fit(X_train, y_train)


# plt.figure(figsize=(20, 12))

# plot_importance(model)
# plt.show()

"""### Checking Variable Correlation

Now we are checking all of the variables within the dataset using a correlation matrix, and we are going to filter out the highly correlated variables.
"""

# correlation_matrix = df.corr()
# plt.figure(figsize=(16, 12))
# sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=.5)

# plt.title('Correlation Matrix')
# plt.show()

"""Based on the Feature Importance graph generated based on our KGBoost model and the correlation matrix, we're informed on which features it would be beneficial to remove.

We remove columns that are:
1. Notably unimportant according to our Feature Importance graph.
2. Highly correlated, according to our Corerlation Matrix.

"""

df = df.drop(['volume_EMA_13', 'volume_EMA_26', 'volume_EMA_54',  'reference_price_EMA_13',
         'reference_price_EMA_26', 'wap_EMA_26', 'wap_EMA_54',
         'vwap_reference_price', 'vwap_matched_size', 'near_price'], axis=1)

"""Now, let's see our correlation matrix again..."""

# correlation_matrix = df.corr()
# plt.figure(figsize=(16, 12))
# sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=.5)

# plt.title('Correlation Matrix')
# plt.show()

"""# Making and Training the Model

For the model, we trained a neural network not covered in class called an LSTM (long short term memory).

Initially we opted to use a FFN, but quickly realised that an FFN would not have any way to recognise time based relationships. Therefore, we researched models that we could use and settled on an LSTM

Base model hyperparameters
"""

SEED = 42
N_LAGS = 55
BATCH_SIZE = 32
BUFFER_SIZE = 100000
EPOCHS = 10
PATIENCE = 25
DROPOUT = 0.5
LEARNING_RATE = 1e-4
SPLIT_DAY = 390
N_STOCKS = 200
N_DATES = 481
N_SECONDS = 55
RUN_TRAINING = True
RUN_FOR_SUBMISSION = True

"""Formats the data to be able to be inserted into the LSTM"""

def windowed_dataset(dataset, shuffle=True):
    dataset = tf.data.Dataset.from_tensor_slices(dataset)
    dataset = dataset.window(N_LAGS + 1, shift=1, drop_remainder=True)
    dataset = dataset.flat_map(lambda window: window.batch(N_LAGS + 1))
    dataset = dataset.map(lambda window: (window[:-1], window[-1]))
    if shuffle:
      dataset = dataset.shuffle(BUFFER_SIZE)
    dataset = dataset.batch(BATCH_SIZE).prefetch(1)
    return dataset


def build_features(df):

    all_stock_ids = range(N_STOCKS)
    all_date_ids = df["date_id"].unique()
    all_seconds = [i * 10 for i in range(N_SECONDS)]

    multi_index = pd.MultiIndex.from_product([all_stock_ids, all_date_ids, all_seconds],
                                             names=['stock_id', 'date_id', 'seconds_in_bucket'])
    df_full = df.set_index(['stock_id', 'date_id', 'seconds_in_bucket']).reindex(multi_index)
    df_full = df_full.fillna(0)
    df_full = df_full.reset_index()

    df_pivoted = df_full.pivot_table(
                values='target',
                index=['date_id', 'seconds_in_bucket'],
                columns='stock_id')

    df_pivoted = df_pivoted.reset_index(drop=True)
    df_pivoted.columns.name = None

    return df_pivoted

"""Builds the model"""

build_features(df)

def build_model(dropout=DROPOUT):
    model = Sequential()
    model.add(Input(shape=(N_LAGS, N_STOCKS)))
    model.add(Dropout(dropout))
    model.add(LSTM(25, return_sequences=False))
    model.add(Dropout(dropout))
    model.add(Dense(N_STOCKS))
    model.compile(loss='mae',
                  optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE))
    return model

"""Trains the base model"""

if RUN_TRAINING:

  split = df['date_id'] > SPLIT_DAY
  df_train = df[~split]
  df_valid = df[split]

  df_train_features = build_features(df_train)
  df_valid_features = build_features(df_valid)

  scaler = StandardScaler()
  train_features = scaler.fit_transform(df_train_features)
  valid_features = scaler.transform(df_valid_features)

  train_dataset = windowed_dataset(train_features)
  valid_dataset = windowed_dataset(valid_features, shuffle=False)

  model = build_model()

  early_stopping = EarlyStopping(monitor='val_loss',
                    mode='min',
                    patience=PATIENCE,
                    restore_best_weights=True,
                    verbose=True)

  history = model.fit(train_dataset,
                      validation_data=valid_dataset,
                      epochs=EPOCHS,
                      batch_size=BATCH_SIZE,
                      callbacks=[early_stopping],
                      verbose=True)

  ## Evaluate ##
  y_pred = model.predict(valid_dataset)

  y_pred = scaler.inverse_transform(y_pred)
  y_true = df_valid_features[N_LAGS:]

  mae = mean_absolute_error(y_true, y_pred)
  print(f"MAE score: {mae}")

"""Plots the base model"""

## Plots ##
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model train vs validation loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper right')
plt.show()

"""Retrains the model using hyperparameter tuning and returns the optimal lowest MAE."""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dropout, LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
import itertools

SEED = 42
N_LAGS = 55
BUFFER_SIZE = 100000
SPLIT_DAY = 390
N_STOCKS = 200
N_SECONDS = 55
RUN_TRAINING = True
PATIENCE = 25
EPOCHS = 10

# Hyperparameters
param_grid = {
    'N_LAGS': [55, 60, 65],
    'BATCH_SIZE': [32, 64],
    'DROPOUT': [0.3, 0.5, 0.7],
    'LEARNING_RATE': [1e-4, 1e-3],
    'LSTM_UNITS': [25, 50]
}

best_mae = float('inf')
best_hyperparameters = None

def windowed_dataset(dataset, shuffle=True, batch_size=BATCH_SIZE):
    dataset = tf.data.Dataset.from_tensor_slices(dataset)
    dataset = dataset.window(N_LAGS + 1, shift=1, drop_remainder=True)
    dataset = dataset.flat_map(lambda window: window.batch(N_LAGS + 1))
    dataset = dataset.map(lambda window: (window[:-1], window[-1]))
    if shuffle:
        dataset = dataset.shuffle(BUFFER_SIZE)
    dataset = dataset.batch(batch_size).prefetch(1)
    return dataset

def build_features(df):
    all_stock_ids = range(N_STOCKS)
    all_date_ids = df["date_id"].unique()
    all_seconds = [i * 10 for i in range(N_SECONDS)]

    multi_index = pd.MultiIndex.from_product([all_stock_ids, all_date_ids, all_seconds],
                                             names=['stock_id', 'date_id', 'seconds_in_bucket'])
    df_full = df.set_index(['stock_id', 'date_id', 'seconds_in_bucket']).reindex(multi_index)
    df_full = df_full.fillna(0)
    df_full = df_full.reset_index()

    df_pivoted = df_full.pivot_table(
        values='target',
        index=['date_id', 'seconds_in_bucket'],
        columns='stock_id')

    df_pivoted = df_pivoted.reset_index(drop=True)
    df_pivoted.columns.name = None

    return df_pivoted

if RUN_TRAINING:
    split = df['date_id'] > SPLIT_DAY
    df_train = df[~split]
    df_valid = df[split]

    df_train_features = build_features(df_train)
    df_valid_features = build_features(df_valid)

    scaler = StandardScaler()
    train_features = scaler.fit_transform(df_train_features)
    valid_features = scaler.transform(df_valid_features)

    train_dataset = windowed_dataset(train_features)
    valid_dataset = windowed_dataset(valid_features, shuffle=False)

    for params in itertools.product(*param_grid.values()):
        hyperparameters = dict(zip(param_grid.keys(), params))

        def build_model(dropout=hyperparameters['DROPOUT'], lstm_units=hyperparameters['LSTM_UNITS']):
            model = Sequential()
            model.add(Input(shape=(hyperparameters['N_LAGS'], N_STOCKS)))
            model.add(Dropout(dropout))
            model.add(LSTM(lstm_units, return_sequences=False))
            model.add(Dropout(dropout))
            model.add(Dense(N_STOCKS))
            model.compile(loss='mae',
                          optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparameters['LEARNING_RATE']))
            return model

        model = build_model()

        early_stopping = EarlyStopping(monitor='val_loss',
                                       mode='min',
                                       patience=PATIENCE,
                                       restore_best_weights=True,
                                       verbose=True)

        history = model.fit(train_dataset,
                            validation_data=valid_dataset,
                            epochs=EPOCHS,
                            batch_size=hyperparameters['BATCH_SIZE'],
                            callbacks=[early_stopping],
                            verbose=True)

        y_pred = model.predict(valid_dataset)

        y_pred = scaler.inverse_transform(y_pred)
        y_true = df_valid_features[N_LAGS:]

        mae = mean_absolute_error(y_true, y_pred)
        print(f"MAE score for hyperparameters {hyperparameters}: {mae}")

        if mae < best_mae:
            best_mae = mae
            best_hyperparameters = hyperparameters

    print(f"Best hyperparameters: {best_hyperparameters}, Best MAE: {best_mae}")